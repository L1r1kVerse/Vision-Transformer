{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 242,
     "status": "ok",
     "timestamp": 1733253340605,
     "user": {
      "displayName": "Kiril Dervishev",
      "userId": "15415381553858752382"
     },
     "user_tz": -120
    },
    "id": "nFo8HHKC3P2B"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yT2tmtL84Xuf"
   },
   "source": [
    "## Layer Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9L_onvS53t7L"
   },
   "source": [
    "Layer normalization is a technique used in deep learning to stabilize and accelerate the training of neural networks by normalizing the inputs to a layer across the features for each individual training example. It is an alternative to batch normalization, which normalizes across the batch dimension, particularly for scenarios where batch statistics are less reliable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BEvfka0U4jZB"
   },
   "source": [
    "##### Key Characteristics of Layer Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M5R6A08-4lho"
   },
   "source": [
    "\n",
    "1. **Normalization Across Features**: Unlike batch normalization, which normalizes across the batch dimension, layer normalization normalizes across the feature dimension for each training example independently.\n",
    "   $$\n",
    "   \\hat{\\mathbf{x}} = \\frac{\\mathbf{x} - \\mu}{\\sigma}\n",
    "   $$\n",
    "   where:\n",
    "   - $\\mathbf{x}$ is the input to the layer.\n",
    "   - $\\mu = \\frac{1}{H} \\sum_{i=1}^H x_i$ is the mean across features.\n",
    "   -$\\sigma = \\sqrt{\\frac{1}{H} \\sum_{i=1}^H (x_i - \\mu)^2 + \\epsilon}$ is the standard deviation across features.\n",
    "   - \\(H\\) is the number of features.\n",
    "\n",
    "2. **Learnable Parameters**: Layer normalization includes learnable scale ($\\gamma$) and shift $(beta)$parameters:\n",
    "   $$\n",
    "   \\mathbf{y} = \\gamma \\cdot \\hat{\\mathbf{x}} + \\beta\n",
    "   $$   These allow the model to restore or adaptively scale the normalized outputs if needed.\n",
    "\n",
    "3. **Independence from Batch Size**: Since layer normalization computes statistics independently for each example, it works well with small batch sizes and is well-suited for recurrent and transformer architectures.\n",
    "\n",
    "### Advantages of Layer Normalization\n",
    "- **Stabilized Training**: By normalizing feature activations, it reduces the risk of exploding or vanishing gradients.\n",
    "- **Independence from Batch Size**: Unlike batch normalization, it does not depend on batch statistics, making it ideal for models trained with small or variable batch sizes.\n",
    "- **Improved Convergence**: It can accelerate training by ensuring more consistent gradients.\n",
    "\n",
    "### Use Cases\n",
    "- Recurrent Neural Networks (RNNs): Layer normalization is often used in RNNs since batch normalization is challenging to apply due to sequential dependencies.\n",
    "- Transformer Models: Transformers (e.g., BERT, GPT) widely use layer normalization due to its efficiency and ability to handle small batch sizes effectively.\n",
    "\n",
    "Layer normalization has become a standard component in many modern deep learning architectures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RVTFEhTtEH86"
   },
   "source": [
    "### Layer Normalization Numerical Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nXZSARF08Aop"
   },
   "source": [
    "\n",
    "---\n",
    "\n",
    "Given a single input vector with 3 features:\n",
    "\n",
    "$\n",
    "\\mathbf{x} = [4.0, 2.0, 8.0]\n",
    "$\n",
    "\n",
    "#### Step 1: Compute the Mean ($\\mu$) and Standard Deviation ($\\sigma$)\n",
    "\n",
    "The mean is calculated as:\n",
    "\n",
    "$\n",
    "\\mu = \\frac{1}{H} \\sum_{i=1}^H x_i = \\frac{4.0 + 2.0 + 8.0}{3} = 4.67\n",
    "$\n",
    "\n",
    "The standard deviation is:\n",
    "\n",
    "$\n",
    "\\sigma = \\sqrt{\\frac{1}{H} \\sum_{i=1}^H (x_i - \\mu)^2}\n",
    "$\n",
    "\n",
    "Substituting the values:\n",
    "\n",
    "$\n",
    "\\sigma = \\sqrt{\\frac{1}{3} \\left((4.0 - 4.67)^2 + (2.0 - 4.67)^2 + (8.0 - 4.67)^2 \\right)}\n",
    "$\n",
    "\n",
    "$\n",
    "\\sigma = \\sqrt{\\frac{1}{3} \\left(0.4489 + 7.1289 + 11.0889\\right)} = \\sqrt{6.89} \\approx 2.63\n",
    "$\n",
    "\n",
    "---\n",
    "\n",
    "#### Step 2: Normalize the Features ($\\hat{x}_i$)\n",
    "\n",
    "The normalized features are computed as:\n",
    "\n",
    "$\n",
    "\\hat{x}_i = \\frac{x_i - \\mu}{\\sigma}\n",
    "$\n",
    "\n",
    "For each feature:\n",
    "\n",
    "$\n",
    "\\hat{x}_1 = \\frac{4.0 - 4.67}{2.63} \\approx -0.25, \\quad\n",
    "\\hat{x}_2 = \\frac{2.0 - 4.67}{2.63} \\approx -1.02, \\quad\n",
    "\\hat{x}_3 = \\frac{8.0 - 4.67}{2.63} \\approx 1.27\n",
    "$\n",
    "\n",
    "Thus:\n",
    "\n",
    "$\n",
    "\\hat{\\mathbf{x}} = [-0.25, -1.02, 1.27]\n",
    "$\n",
    "\n",
    "---\n",
    "\n",
    "#### Step 3: Apply Scale ($\\gamma$) and Shift ($\\beta$)\n",
    "\n",
    "Assume the learnable parameters are:\n",
    "\n",
    "$\n",
    "\\gamma = [1.5, 1.0, 0.5], \\quad \\beta = [0.5, 0.0, -0.5]\n",
    "$\n",
    "\n",
    "The final output is computed as:\n",
    "\n",
    "$\n",
    "y_i = \\gamma_i \\cdot \\hat{x}_i + \\beta_i\n",
    "$\n",
    "\n",
    "For each feature:\n",
    "\n",
    "$\n",
    "y_1 = 1.5 \\cdot -0.25 + 0.5 = 0.125, \\quad\n",
    "y_2 = 1.0 \\cdot -1.02 + 0.0 = -1.02, \\quad\n",
    "y_3 = 0.5 \\cdot 1.27 - 0.5 = 0.135\n",
    "$\n",
    "\n",
    "Thus, the final output is:\n",
    "\n",
    "$\n",
    "\\mathbf{y} = [0.125, -1.02, 0.135]\n",
    "$\n",
    "\n",
    "---\n",
    "\n",
    "#### Summary\n",
    "\n",
    "1. **Input**: $[4.0, 2.0, 8.0]$\n",
    "2. **Normalized**: $[-0.25, -1.02, 1.27]$\n",
    "3. **Final Output After Scale and Shift**: $[0.125, -1.02, 0.135]$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jBc42sNX8D9_"
   },
   "source": [
    "### Layer Normalization PyTorch Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1733254582453,
     "user": {
      "displayName": "Kiril Dervishev",
      "userId": "15415381553858752382"
     },
     "user_tz": -120
    },
    "id": "40fVAFyh_UDP",
    "outputId": "67417978-0571-4c90-c742-2d608244fca3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: tensor([[4., 2., 8.]])\n",
      "Normalized Output: tensor([[ 0.0991, -1.0690,  0.1682]], grad_fn=<NativeLayerNormBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Input vector\n",
    "x = torch.tensor([[4.0, 2.0, 8.0]])  # Shape [1, 3] (1 sample, 3 features)\n",
    "\n",
    "# Define a LayerNorm instance with normalized dimension matching the input\n",
    "layer_norm = nn.LayerNorm(normalized_shape=x.size()[1])\n",
    "\n",
    "# Manually set the learnable parameters (gamma and beta) to match the example\n",
    "with torch.no_grad():\n",
    "    layer_norm.weight = nn.Parameter(torch.tensor([1.5, 1.0, 0.5]))  # Gamma (scale)\n",
    "    layer_norm.bias = nn.Parameter(torch.tensor([0.5, 0.0, -0.5]))   # Beta (shift)\n",
    "\n",
    "# Apply the layer normalization\n",
    "y = layer_norm(x)\n",
    "\n",
    "print(\"Input:\", x)\n",
    "print(\"Normalized Output:\", y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uwmlLtNeCZ66"
   },
   "source": [
    "PyTorch uses a sligthly different implementation to enhance numerical stability and this causes the outputs to be slightly different than the outputs in our numerical example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I5gOI26BDOFc"
   },
   "source": [
    "### **Main Function of Layer Normalization**\n",
    "1. **Stabilizing Activations**:  \n",
    "   By normalizing the input features for each sample to have a consistent scale and mean, layer normalization ensures that the activations passed to subsequent layers remain in a stable range. This makes optimization smoother, as the learning dynamics are less affected by variations in feature magnitudes.\n",
    "\n",
    "2. **Reducing Internal Covariate Shift**:  \n",
    "   Internal covariate shift refers to the change in the distribution of layer inputs during training as the parameters of preceding layers change. Layer normalization mitigates this by keeping the inputs to each layer in a more predictable range, reducing the amount of \"re-adjustment\" needed by subsequent layers.\n",
    "\n",
    "3. **Independence from Batch Size**:  \n",
    "   Unlike batch normalization, which relies on statistics computed across a batch, layer normalization computes normalization statistics per sample. This makes it particularly useful in models with:\n",
    "   - Small batch sizes (e.g., recurrent neural networks, transformers).\n",
    "   - Applications like reinforcement learning or language modeling where batch normalization may not be practical.\n",
    "\n",
    "---\n",
    "\n",
    "### **Secondary Benefits**\n",
    "1. **Mitigating Gradient Problems**:\n",
    "   - **Gradient Explosion**: Normalization ensures that extremely large activations are scaled down, indirectly reducing the risk of exploding gradients.\n",
    "   - **Gradient Vanishing**: By maintaining consistent feature scales, layer norm helps prevent activations from becoming too small to propagate meaningful gradients.\n",
    "\n",
    "2. **Faster Convergence**:  \n",
    "   The smoother learning dynamics result in faster convergence during training, often requiring fewer iterations to achieve good performance.\n",
    "\n",
    "---\n",
    "\n",
    "### When to Use Layer Normalization\n",
    "Layer normalization is particularly effective in models where:\n",
    "- **Sequences or temporal structures** are critical (e.g., RNNs, Transformers).  \n",
    "- Batch normalization doesn't work well due to **variable batch sizes** or **dependencies across samples**.  \n",
    "\n",
    "In these scenarios, layer normalization not only stabilizes training but also ensures consistent and robust gradients, indirectly avoiding numerical issues like gradient explosion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xTBDY5HDEchE"
   },
   "source": [
    "### Conclusion\n",
    "\n",
    "Layer normalization essentially normalizes the inputs with the z-score for the features of each sample, plus learnable scale and shift parameters.The learnable\n",
    "𝛾\n",
    "γ and\n",
    "𝛽\n",
    "β provide flexibility so the network can learn optimal representations rather than being constrained by strict normalization."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNS4wOUR00TUWr8oGJrM5c2",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
