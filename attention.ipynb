{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single Head Attention in Numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let as assume we have already split the input image into patches, linearly projected them, prepended the class embedding and added the position embedded as discussed in the main notebook for the project - vision_transformer.ipynb. Now we have an input tensor of dimensions:<br>[1, 197, 768] <br>\n",
    "[batch_size, number_of_patches, embedding_dimension].\n",
    "\n",
    " Below we will implement the attention mechanism in a single attention head only using NumPy and np.arrays instead of tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.0662793 , 0.20086902, 0.88743186, ..., 0.02730168,\n",
       "         0.70829993, 0.87831014],\n",
       "        [0.5703314 , 0.26718548, 0.51736575, ..., 0.8105333 ,\n",
       "         0.5822567 , 0.6799193 ],\n",
       "        [0.73056495, 0.03794412, 0.8885933 , ..., 0.08197571,\n",
       "         0.8888563 , 0.6545716 ],\n",
       "        ...,\n",
       "        [0.8376981 , 0.43273795, 0.10236892, ..., 0.27505141,\n",
       "         0.08347566, 0.32058024],\n",
       "        [0.5240691 , 0.70952046, 0.05152313, ..., 0.93939435,\n",
       "         0.40975556, 0.4971096 ],\n",
       "        [0.50677985, 0.9401297 , 0.0452047 , ..., 0.8495476 ,\n",
       "         0.6295712 , 0.20522721]]], dtype=float32)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set input shape\n",
    "input_shape = (1, 197, 768)\n",
    "# Create the embedded patches\n",
    "patch_embeddings = np.random.uniform(0.0, 1.0,size = input_shape).astype(np.float32)\n",
    "patch_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of projection weights is (768, 64)\n",
      "Shape of Q, K and V is: (1, 197, 64)\n",
      "Shape attention score is: (1, 197, 197)\n",
      "Shape attention weights is: (1, 197, 197)\n",
      "Shape of attention_output is: (1, 197, 64)\n",
      "Shape of input  is: (1, 197, 768)\n",
      "Shape of output after projection is: (1, 197, 768)\n"
     ]
    }
   ],
   "source": [
    "# Input dimensions\n",
    "batch_size, num_patches, embedding_dim = patch_embeddings.shape\n",
    "\n",
    "\n",
    "# Set query/key dimension (usually embedding dimension / number of heads)\n",
    "d_k = embedding_dim // 12\n",
    "W_q = np.random.randn(embedding_dim, d_k).astype(np.float32) * 0.1\n",
    "W_k = np.random.randn(embedding_dim, d_k).astype(np.float32) * 0.1\n",
    "W_v = np.random.randn(embedding_dim, d_k).astype(np.float32)* 0.1\n",
    "W_o = np.random.randn(d_k, embedding_dim).astype(np.float32) * 0.1\n",
    "print(f\"Shape of projection weights is {W_q.shape}\")\n",
    "\n",
    "\n",
    "# Linear projections for Q, K, and V\n",
    "# Here each 2d slice of the 3d input is multiplied with the weight matrices\n",
    "Q = patch_embeddings @ W_q # Shape: (batch, number of patches, d_k)\n",
    "K = patch_embeddings @ W_k # Shape: (batch, number of patches, d_k)\n",
    "V = patch_embeddings @ W_v # Shape: (batch, number of patches, d_k)\n",
    "print(f\"Shape of Q, K and V is: {Q.shape}\")\n",
    "\n",
    "# Calculate attention scores\n",
    "attention_scores = Q @ K.transpose(0, 2, 1) / np.sqrt(d_k) # shape is (1, 187, 187) [batch_number_queries, number_keys]\n",
    "print(f\"Shape attention score is: {attention_scores.shape}\")\n",
    "\n",
    "# Apply softmax\n",
    "attention_weights = np.exp(attention_scores) / np.sum(np.exp(attention_scores), axis = -1, keepdims = True)\n",
    "print(f\"Shape attention weights is: {attention_weights.shape}\")\n",
    "\n",
    "# Weight values\n",
    "attention_output = attention_weights @ V\n",
    "\n",
    "print(f\"Shape of attention_output is: {attention_output.shape}\")\n",
    "# Output projection\n",
    "output = attention_output @ W_o\n",
    "print(f\"Shape of input  is: {patch_embeddings.shape}\")\n",
    "print(f\"Shape of output after projection is: {output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that for the attention layer the shape of the output matches the shape of the input."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
